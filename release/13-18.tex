%Назарова

\chapter*{Глава 3: Обозначение О-большое}

\vspace{\baselineskip}

\textbf{Определение}

\vspace{\baselineskip}
В основе обозначения О-большое лежит математическая форма записи, используемая для сравнения скорости сходимости функций. Пусть n -> f(n) и n -> g(n) будут функциями, определёнными над натуральными числами. Cкажем, что f = O(g) тогда и только тогда, когда f(n)/g(n) ограничена и n стремится к бесконечности. Другими словами, f = O(g), тогда и только тогда, когда существует  константа A, такая, что для всех n, f(n)/g(n) $\le$ A.

\vspace{\baselineskip}
На самом деле область применения обозначения О-большое немного шире в математике, но для лучшего понимания  я упростил её до необходимых  в анализе сложности алгоритма понятий: функции, определённые на натуральных величинах, которые имеют ненулевые значения, и случаи, когда n растёт до бесконечности.

\vspace{\baselineskip}
\textbf{Что это означает?}

\vspace{\baselineskip}
Рассмотрим случай f(n) = 100$n^{\p{2}}$ + 10n + \p{1} и g(n) = $n^{\p{2}}$. Совершенно очевидно, что обе эти функции стремятся к бесконечности при n стремящемся к бесконечности. Но иногда недостаточно знать предел, мы также хотим знать и \textit{скорость}, с которой функции стремятся к своему пределу. Такие понятия, как О-большое, помогают сравнивать и классифицировать функции по характеру их изменения.

\vspace{\baselineskip}
Давайте выясним, что если f = O(g)  по определению, тогда выходит, что f(n)/g(n) = \p{100} + \p{10}/n + \p{1}/$n^{\p{2}}$. Так как \p{10}/n равно 10, когда n равно 1 и уменьшается, также \p{1}/$n^{\p{2}}$ равно 1, когда n равно 1 и уменьшается при росте n, тогда получаем f(n)/g(n) <= \p{100} + \p{10} + \p{1} = \p{111}. Это удовлетворяет определению,  так как мы нашли границу f(n)/g(n) (111) и поэтому f = O(g) (скажем, что f --- это О-большое от $n^{\p{2}}$). 

\vspace{\baselineskip}
Это значит, что f стремится к бесконечности примерно с той же скоростью, что и g. Это может показаться странным, потому что мы обнаружили, что f в 111 раз больше, чем g, или другими словами, когда g вырастает на 1, f вырастает максимум на 111. Может показаться, что рост в 111 раз быстрее это не "примерно та же скорость". Действительно, О-большое --- не очень точный способ классификации скорости сходимости функций, вот почему в математике мы используем \href{https://en.wikipedia.org/wiki/Asymptotic_analysis}{\underline{отношение эквивалентности}}, когда нам нужна точная оценка. Однако для разделения алгоритмов в больших классах скоростей достаточно О-большого. Нам не нужно разделять функции, которые растут в фиксированное количество раз быстрее друг друга, а только те, которые растут бесконечно быстрее друг друга.Например, если взять h(n) = $n^{\p{2}}$*log(n), то мы увидим, что h(n)/g(n) = log(n), который стремится к бесконечности с увеличением n, поэтому h \textit{не} является O($n^{2}$), так как h растет \textit{бесконечно} быстрее, чем $n^{2}$.

\vspace{\baselineskip}
Теперь мне нужно сделать замечание: вы могли заметить, что если f = O(g) и g = O(h), то f = O(h). Например, в нашем случае имеем f = O($n^{\p{3}}$) и f = O($n^{\p{4}}$)... В анализе сложности алгоритмов мы часто говорим f = O(g), что означает f = O(g) \textit{и} g = O(f), это можно понимать как "g --- самое маленькое О-большое для f". В математике мы говорим, что такие функции --- это Тета-большие друг друга.

\newpage
\textbf{Как это используется?}

\vspace{\baselineskip}
При сравнении производительности алгоритмов нас интересует количество операций, выполняемых алгоритмом. Это называется \textit{временной сложностью}. В данной модели мы считаем, что каждая основная операция (сложение, умножение, сравнение, присваивание и т.д.) занимает фиксированное количество времени, и подсчитываем количество таких операций. Обычно мы можем выразить это число как функцию размера входного значения, которую мы называем n. И, к сожалению, это число обычно вырастает до бесконечности с n (если это не так, то мы говорим, что алгоритм --- O(1)). Мы разделяем наши алгоритмы на классы больших скоростей, определяемые О-большое: когда мы говорим об "алгоритме O($n^{2}$)", то имеем в виду, что количество выполняемых им операций, выраженное в функции от n, равно O($n^{2}$). Это говорит о том, что наш алгоритм примерно так же быстр, как и алгоритм, который бы выполнял ряд операций, равных квадрату размера его входного значения, или даже \textit{быстрее} его. Я говорю “даже быстрее его”, потому что я использовал О-большое вместо Тета-большое, но обычно люди говорят О-большое, подразумевая Тета-большое.

\vspace{\baselineskip}
При подсчете операций обычно учитывается наихудший случай: например, если у нас есть цикл, который может выполняться в большинстве случаев и содержит 5 операций, то количество операций, которое мы считаем, составляет 5n. 

\vspace{\baselineskip}
Краткое примечание: быстрый алгоритм --- это алгоритм, который выполняет несколько операций, поэтому если количество операций \textit{быстрее} возрастает до бесконечности, то алгоритм работает \textit{медленнее}: O(n) лучше, чем O($n^{2}$).

\vspace{\baselineskip}
Нас также иногда интересует \textit{пространственная сложность} нашего алгоритма. Для этого мы рассматриваем количество байт в памяти, занимаемое алгоритмом, как функцию от размера входа, и используем Big-O таким же образом.


\section*{Раздел 3.1: Простой цикл}

Следующая функция находит максимальный элемент в массиве:

\vspace{\baselineskip}
\begin{tcolorbox}
\begin{minted}{C}

//int перед названием ф-ии означает, что ф-ия возвращает значение типа int

int find_max(const int *array, size_t len) { 
	//создание функции find_max,которая находит максимальный элемент в массиве 
	//array длины len
	int max = INT_MIN; //переменной max задаётся минимально возможное значение,
	//сама переменная будет содержать макс эл-т в конце выполнения ф-ии
	for (size_t i = 0; i < len; i++) {
		//проходим по массиву сравнивая эл-ты с максимальным
		if (max < array[i]) {
		max = array[i];
	  //присваиваем max значение больших эл-ов тем самым находим максимальный эл-т
		}
	}
	return max;
}
\end{minted}
\end{tcolorbox}

\vspace{\baselineskip}
Входной размер --- это размер массива, который я назвал len в коде. 

\vspace{\baselineskip}
Давайте посчитаем операции.

\vspace{\baselineskip}
\begin{tcolorbox}
\begin{minted}{C}
int max = INT_MIN;
size_t i = 0;	 // из цикла for
\end{minted}
\end{tcolorbox}


\vspace{\baselineskip}
Эти два задания выполняются только один раз, следовательно это 2 операции. Зацикленные операции:

\vspace{\baselineskip}
\begin{tcolorbox}
\begin{minted}{C}
if (max < array[i])// 1
	i++;				// 2
max = array[i]	  // 3
\end{minted}
\end{tcolorbox}

\vspace{\baselineskip}
Так как в цикле 3 операции, а цикл выполняется n раз, то мы добавляем 3n к уже существующим 2 операциям, чтобы получить 3n + \p{2}. Таким образом, наша функция принимает 3n + \p{2} операции для нахождения максимума (его сложность 3n + \p{2}). Это многочлен, где самым быстрорастущим членом является фактор n, так что это сложность  O(n).

\vspace{\baselineskip}
 Вы, наверное, заметили, что "операция" не очень хорошо определена. Например, я сказал, что \textbf{если} (max < array[i]) --- это одна операция, но в зависимости от архитектуры этот оператор может скомпилироваться, например, в три инструкции: одна операция чтения памяти, одна операция сравнения и одна ветка. Я также считал все операции одинаковыми, даже несмотря на то, что, например, операции с памятью будут медленнее других, и их производительность будет варьироваться в больших пределах, например, для эффектов кэширования. Я также полностью проигнорировал операцию возврата, тот факт, что для функции будет создан фрейм и так далее.В конце концов, для анализа сложности это не имеет значения, потому что, каким бы способом я ни выбрал подсчет операций, он изменит только коэффициент n и константу, так что результат все равно будет O(n). Сложность показывает, как алгоритм меняется с размером входного значения, но это не единственный аспект производительности!
 
\vspace{\baselineskip}
Очевидно, что во втором случае выполняется меньше операций,и поэтому он более эффективен. Как это интерпретируется на Big-O нотацию? Ну,а теперь внутренний цикл тела выполняется \p{1} + \p{2} + ... + n - \p{1} = n(n-\p{1})/\p{2} раз. Это \textit{все еще} многочлен от
второй степени, и поэтому все еще лишь O($n^{\p{2}}$). Мы явно снизили сложность, так как грубо разделили на
2 количество операций, которые выполняем, но мы все еще находимся в том же классе сложности --- Big-O.Для того, 
чтобы понизить сложность до более низкого класса, нам нужно было разделить количество операций на то, что
стремится к \textit{бесконечности} с n.

\vspace{\baselineskip}

\section*{Раздел 3.2: Вложенный цикл}

\vspace{\baselineskip}
Следующая функция проверяет, есть ли в массиве дубликаты, выбирая каждый элемент, а затем проводя итерации по всему массиву, чтобы увидеть, есть ли в нем этот элемент

\vspace{\baselineskip}
\begin{tcolorbox}
\begin{minted}{C}
_Bool contains_duplicates(const int *array, size_t len) {
	// _Bool - возвращаемый тип функции
	for (int i = 0; i < len - 1; i++) {
		for (int j = 0; j < len; j++) {
			// цикл по очереди сравнивает каждый элемент кроме последнего 
			// со всеми эл-ами
			if (i != j && array[i] == array[j]) {
				return 1;
				// возвращает 1, если вдруг находит равные элементы из
				// разных ячеек массива
			}
		}
	}
	return 0;
}

\end{minted}
\end{tcolorbox}

\vspace{\baselineskip}
Внутренний цикл выполняет на каждой итерации несколько постоянных операций с n. Внешний цикл также выполняет несколько постоянных операций и выполняет внутренний цикл n раз. Сам внешний цикл выполняется n раз. Таким образом, операции внутри внутреннего цикла выполняются $n^{\p{2}}$ раз, во внешнем --- n раз, а присваивание i выполняется один раз. Таким образом, сложность будет примерно равна a$n^{\p{2}}$+ bn + c, а так как наивысший член --- $n^{\p{2}}$, то обозначение O - O($n^{\p{2}}$). 

\vspace{\baselineskip}
Как вы, возможно, заметили, мы можем улучшить алгоритм, не делая одни и те же сравнения несколько раз. Мы можем начать с i + \p{1} во внутреннем цикле, потому что все элементы до него уже были проверены на наличие элементов массива, в том числе и с индексом i + \p{1}. Это позволяет отказаться от проверки i == j.

\vspace{\baselineskip}
\begin{tcolorbox}
\begin{minted}{C}
_Bool faster_contains_duplicates(const int *array, size_t len) {
	// len - длина данного массива
	for (int i = 0; i < len - 1; i++) {
		for (int j = i + 1; j < len; j++) {
			// тот же цикл что и предыдущий, но сравнивает каждый элемент со всеми
			// последующими эл-ами массива
			if (array[i] == array[j]) {
				return 1;
			}
		}
	}
	return 0;
}

\end{minted}
\end{tcolorbox}

\vspace{\baselineskip}
\section*{Раздел 3.3: Типы алгоритмов O(log n)}

\vspace{\baselineskip}
Допустим, у нас есть сложность размера n. Теперь для каждого шага нашего алгоритма (который нам нужно написать), наша исходная сложность становится вдвое меньше предыдущей  (n/2).

\vspace{\baselineskip}
Таким образом, на каждом шаге наша задача становится вдвое меньше.

\vspace{\baselineskip}

\textbf{Step Problem}\newline
1 \hspace{6,5mm} n/2 \newline
2 \hspace*{6,5mm} n/4 \newline
3 \hspace*{6,5mm} n/8 \newline
4 \hspace*{6,5mm} n/16 \newline


Когда  требуемое пространство сокращено (т.е. задача решена полностью), уменьшение становится невозможным (n становится равным 1) после выхода из условия проверки.


\begin{enumerate}
    \item Поговорим о k-ом шаге или количестве операций:
    
    \vspace{\baselineskip}
    \textbf{сложность-задачи} = 1
    
    \item Но мы знаем ,что на k-ом шаге сложность-задачи должна быть:
    
    \vspace{\baselineskip}
    \textbf{сложность-задачи} = n/2k
    
    \item От 1 и 2:
    
    \vspace{\baselineskip}
    n/2k = 1 или
    
    \vspace{\baselineskip}
    n = 2k
    
    \item Возьмем логарифм от обеих частей
    
    \vspace{\baselineskip}
    loge n = k loge2
    
    \vspace{\baselineskip}
    или
    
    \vspace{\baselineskip}
    k = loge n / loge 2
    
    \item Воспользуемся формулой \textbf{logx m / logx n = logn m}
    
    \vspace{\baselineskip}
    k = log2
    
    \vspace{\baselineskip}
    или проще  k = log n
\end{enumerate}

\vspace{\baselineskip}
\pline{0.4}{
Теперь мы знаем, что наш алгоритм может работать максимум до log n, следовательно, сложность по времени будет O( log n)}

\vspace{\baselineskip}
Вот очень простой пример кода, дополняющий приведенный выше текст:

\vspace{\baselineskip}
\begin{tcolorbox}
\begin{minted}{C}
for(int i=1; i<=n; i=i*2)
{
	// здесь может быть выполнена какая-либо операция
}
\end{minted}
\end{tcolorbox}

\vspace{\baselineskip}
Так что теперь, если кто-то спросит, сколько шагов выполнит цикл с n=256(или любой другой алгоритм, уменьшающий размер задачи вдвое), то вы можете очень легко вычислить.

\vspace{\baselineskip}

k = log2 256\newline


k = log2 2 8 ( => logaa = 1)\newline


k = 8\newline


Другой очень хороший пример для подобного случая - \textbf{алгоритм двоичного поиска}.

\begin{tcolorbox}
\begin{minted}{C}

int bSearch(int arr[],int size,int item){
	// Бинарный поиск. item - эл-т который ищем, 
	// size - размер массива(кол-во эл-ов), arr[] - массив с которым работаем
	int low=0; // левая граница эл-ов в которых ведём поиск
	int mid; // номер проверяемого эл-та
	int high=size-1; // правая граница
	while(low<=high){ // пока не кончатся эл-ты
		mid=low+(high-low)/2; // берется средний элемент между low и high
		if(arr[mid]==item) // если эл-т массива - искомый
			return mid; // возвращаем номер(итерационный номер в	 массиве)
		else if(arr[mid]<item) // если эл-т массива меньше чем нужно нам
			low=mid+1; // передвигаем левую границу вправо
		else high=mid-1; // иначе двигаем правую границу влево

	}
	return -1; // Неудачный результат
}
\end{minted}
\end{tcolorbox}

\section*{Раздел 3.4: Пример O(log n)}

\vspace{\baselineskip}
\textbf{Предисловие}

\vspace{\baselineskip}
Рассмотрим следующую проблему:

\vspace{\baselineskip}
L --- отсортированный список, содержащий n подписанных целых чисел (n достаточно большое), например [-{\color{Red}{5}}, {\color{Red}{-2}}, {\color{Red}{-1}}, {\color{Red}{0}}, {\color{Red}{1}}, {\color{Red}{2}}, {\color{Red}{4}}] (здесь n имеет значение 7). Если известно, что L содержит целое число 0, то как найти индекс 0 ?

\vspace{\baselineskip}
\textbf{Примитивный подход}

\vspace{\baselineskip}
Первое, что приходит на ум, это просто читать каждый индекс до тех пор, пока не будет найден 0. В худшем случае, количество операций равно n, поэтому сложность --- O(n). 

\vspace{\baselineskip}
Это хорошо работает для малых значений n. Но есть ли более эффективный способ ?

\vspace{\baselineskip}
\textbf{Дихотомия}

\vspace{\baselineskip}
Рассмотрим следующий алгоритм (Python3):

\vspace{\baselineskip}
\begin{tcolorbox}
\begin{minted}{Python}
a = 0
b = n-1
while True:
	h = (a+b)/2 # делим нацело поэтому h будет целым
	if L[h] == 0:
		return h
	elif L[h] > 0: # если э-т под номером h больше нуля
		b = h # двигаем правую границу влево
	elif L[h] < 0: # если э-т под номером h меньше нуля
		a = h # двигаем левую границу вправо
\end{minted}
\end{tcolorbox}


a и b --- это индексы, между которыми находится 0. Каждый раз, когда мы входим в цикл, мы выбираем индекс между a и b и используем его, чтобы сократить область поиска. 

\vspace{\baselineskip}
В худшем случае, цикл работает до тех пор, пока a и b не будут равны. Но сколько для этого нужно операций? Не n, потому что каждый раз, когда мы входим в цикл, то делим расстояние между a и b примерно двое. Таким образом, сложность представляет собой O(logn).

\vspace{\baselineskip}
\textbf{Пояснение}

\vspace{\baselineskip}
\textit{Примечание: Когда мы пишем "log", то имеем в виду двоичный логарифм, или базу логарифма 2 (в которой мы напишем "$\log_{2}$"). В качестве O($\log_{2}{n}$) = O(logn)  мы будем использовать "log" вместо "$\log_{2}$".}

\vspace{\baselineskip}
Назовем х количеством операций: мы знаем, что 1 = n / ($2^{x}$) . 

\vspace{\baselineskip}
Тогда $2^{x}$ = n,  значит x = log n.

\vspace{\baselineskip}
\textbf{Вывод}

\vspace{\baselineskip}
Когда сталкиваешься с последовательным делением (на два или на любое число), помни, что сложность логарифмическая.

