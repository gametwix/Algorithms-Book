% Моисеенков

\chapter*{Глава 27: Онлайн алгоритмы}

\textbf{Теория:}

\vspace{\baselineskip}

\textbf{Определение 1}: \textbf{Задача оптимизации} $\Pi$ состоит из набора \textbf{экземпляров} $\sum \Pi$. Для каждого экземпляра $\sigma \in \sum\Pi$ существует множество $Z\sigma$ \textbf{решений} и \textbf{целевая функция} \\ $f\sigma:Z\sigma\rightarrow\mathcal{R}\geq0$, которая присваивает каждому решению положительное вещественное значение. Мы говорим, что $OPT(\sigma)$ - это значение оптимального решения, $A(\sigma)$ - это решение алгоритма $A$ для задачи $\Pi$ и $wA(\sigma)=f\sigma(A(\sigma))$ - его значение.

\vspace{\baselineskip}

\textbf{Определение 2}: Онлайн алгоритм $A$ для задачи минимизации $\Pi$ имеет \textbf{конкурентное соотношение} $r \geq 1$, если имеется постоянная $\tau\in\mathcal{R}$ такая, что

\vspace{\baselineskip}

\pline{0.4}{$wA(\sigma)=f\sigma(A(\sigma))\leq r\cdot OPT(\sigma)+\tau$}

\vspace{\baselineskip}

для всех экземпляров $\sigma\in\sum\Pi$. $A$ называется \textbf{r-конкурентным} онлайн алгоритмом. Если даже

\vspace{\baselineskip}

\pline{0.4}{$wA(\sigma)\leq r\cdot OPT(\sigma)$}

\vspace{\baselineskip}

для всех экземпляров $\sigma\in\sum\Pi$, тогда $A$ называется \textbf{строго r-конкурентным} онлайн алгоритмом.

\vspace{\baselineskip}

\textbf{Теорема 1.3}: \textbf{LRU} и \textbf{FWF} являются алгоритмами маркировки.

\vspace{\baselineskip}

\textbf{Доказательство:} В начале каждой фазы (кроме первой) \textbf{FWF} имеет кэш-промах и очищает кэш. Это означает, что у нас есть k пустых страниц. В каждой фазе максимум k различных запрашиваемых страниц, так что на этой фазе будет исключение. Значит, \textbf{FWF} - это алгоритм маркировки.\\
Предположим, что \textbf{LRU} не является алгоритмом маркировки. Тогда есть экземпляр $\sigma$, где \textbf{LRU} помеченная страница x в фазе i исключена. Пусть $\sigma t$ сделает запрос в фазе i, где x исключена. Так как x отмечен, то должен быть более ранний запрос $\sigma t^*$ для x в той же фазе, так что $t^*<t$. После того, как $t^* x$ стала самой новой страницей кэша, для того, чтобы она была исключена из $t$, последовательность $\sigma t^*+1,\dots,\sigma t$ должна запрашивать по крайней мере k из x разных страниц. Это означает, что фаза i запросила по крайней мере k+1 разных страниц, что противоречит определению фазы. Поэтому \textbf{LRU} должен быть алгоритмом маркировки.

\vspace{\baselineskip}

\textbf{Теорема 1.4}: Каждый алгоритм маркировки \textbf{строго k-конкурентный}.

\vspace{\baselineskip}

\textbf{Доказательство:} Пусть $\sigma$ - экземпляр для задачи пейджирования и $l$ - число фаз для $\sigma$. Если $l = 1$, то каждый алгоритм маркировки оптимален, и оптимальный автономный алгоритм не может быть лучше. \\
Мы предполагаем, что $l \geq 2$. Стоимость каждого алгоритма разметки, например $\sigma$, ограничена сверху $l \cdot k$, потому что на каждой фазе алгоритм разметки не может исключать более $k$ страниц без удаления одной помеченной страницы. \\
Теперь мы пытаемся показать, что оптимальный автономный алгоритм исключает, по крайней мере, $k+l-2$ страницы для $\sigma$, $k$ на первой фазе и, по крайней мере, по одной странице на каждой последующей фазе, кроме последней. Для доказательства определим $l-2$ дизъюнктивные подпоследовательности $\sigma$. \\
Последовательность $i \in {1,\dots,l-2}$ начинается со второй позиции фазы i+1 и завершается первой позицией фазы i+2. \\
Пусть x будет первой страницей фазы i+1. В начале подпоследовательности i находится страница x и не более чем k-1 различных страниц в кэше оптимальных автономных алгоритмов. В подпоследовательности i k-й запрос страницы отличается от x, поэтому оптимальный автономный алгоритм должен исключить хотя бы одну страницу для каждой подпоследовательности. Так как на начальной фазе 1 кэш все еще пустой, оптимальный автономный алгоритм вызывает k исключений на первой фазе. Это показывает, что

\vspace{\baselineskip}

\pline{0.4}{$wA(\sigma)\leq l\cdot k \leq (k+l-2)k \leq OPT(\sigma) \cdot k$}

\vspace{\baselineskip}

\textbf{Следствие 1.5}: \textbf{LRU} и \textbf{FWF} являются \textbf{строго k-конкурентными}.

\vspace{\baselineskip}

Если нет константы r, для которой онлайн алгоритм A является r-конкурентным, мы называем A \textbf{неконкурентным}.

\vspace{\baselineskip}

\textbf{Теорема 1.6}: \textbf{LFU} и \textbf{LIFO не являются конкурентными}.

\vspace{\baselineskip}

\textbf{Доказательство:} Пусть $l \geq 2$ - постоянная, $k \geq 2$ - размер кэша. Различные страницы кэша имеют нумерацию $1,\dots,k+1$. Мы смотрим на следующую последовательность:

\vspace{\baselineskip}

\pline{0.4}{\Large$\sigma = (1^l, 2^l, \dots, (k-1)^l, (k, k+1)^{l-1}$}

\vspace{\baselineskip}

Первая страница 1 запрашивается l раз, потом страница 2 и так далее. В конце концов у нас есть (l-1) чередующихся запросов для страницы k и k+1.\\
\textbf{LFU} и \textbf{LIFO} заполняют свой кэш страницами 1-k. Когда запрашивается страница k+1 страница k исключается и наоборот. Это означает, что каждый запрос подпоследовательности \\ (k,k+1)l-1 вызывает одну страницу. Кроме того, при первом использовании страниц 1-(k-1) происходит кэширование k-1. Таким образом, \textbf{LFU} и \textbf{LIFO} исключают k-1+2(l-1) страниц. \\
Теперь нужно показать, что для каждой константы $\tau \in \mathcal{R}$ и каждой константы $r \leq 1$ существует l такое, что

\vspace{\baselineskip}

\pline{0.4}{\Large$w_{LFU}(\sigma) = w_{LIFO}(\sigma) > r \cdot OPT(\sigma) + \tau$}

\vspace{\baselineskip}

что эквивалентно

\vspace{\baselineskip}

\pline{0.4}{\Large$k - 1 + 2(l - 1) > r(k + 1) + \tau \Leftrightarrow l \geq 1 + \frac{r \cdot (k + 1) + \tau - k + 1}{2}$}

\vspace{\baselineskip}

Чтобы это неравенство выполнилось, придется выбрать достаточно большое l. Таким образом, \textbf{LFU} и \textbf{LIFO} не являются конкурентными.

\vspace{\baselineskip}

\newpage

\textbf{Теорема 1.7}: \textbf{Не существует r-конкурентного} детерминированного онлайн алгоритма для пейджирования с \textbf{r < k}.

\vspace{\baselineskip}

\textbf{Используемые источники} \\
\textbf{Основной материал}

\begin{enumerate}
    \item Онлайн алгоритмы скриптов (на немецком языке), Хайко Реглин, Боннский университет.
    \item \href{https://en.wikipedia.org/wiki/Page_replacement_algorithm}{\underline{Алгоритм замены страницы}}
\end{enumerate}

\textbf{Для дальнейшего чтения}

\begin{enumerate}
    \item \href{http://www.cs.technion.ac.il/~rani/book.html}{\underline{Онлайн-вычисления и конкурентный анализ}} Аллана Бородина и Ран Эль-Янива
\end{enumerate}

\textbf{Исходный код}

\begin{enumerate}
    \item Исходный код  \href{https://pastebin.com/AF7EC2xJ}{\underline{автономного кеширования}}
    \item Исходный код  \href{https://pastebin.com/u/kgoedde/1/Wak9refA}{\underline{игры оппонента}}
\end{enumerate}

\section*{Раздел 27.1: Пейджирование (онлайн кэширование)}

\textbf{Предисловие}

\vspace{\baselineskip}

Вместо того, чтобы начинать с формальных определений, подход к теме будет сопровождаться рядом примеров, а определения будут вводиться по пути. Раздел примечаний \textbf{Теория} будет состоять из всех определений, теорем и следствий, чтобы дать вам всю информацию для более быстрого поиска конкретных аспектов.

\vspace{\baselineskip}

Источники в разделе примечаний состоят из базового материала, используемого для написания главы и дополнительного материала для дальнейшего чтения. Вдобавок вы здесь также найдете примеры исходных кодов. Пожалуйста, обратите внимание, что для того, чтобы исходный код приведенных примеров был менее громоздкий, он воздерживается от таких вещей как обработка ошибок и т.д. Он также опускает некоторые специфические особенности языка, которые могут испортить ясность примера, такие как широкое использование расширенных библиотек и т.д.

\vspace{\baselineskip}

\textbf{Пейджирование}

\vspace{\baselineskip}

Проблема пейджирования возникает из-за ограничения конечного
пространства. Предположим, что в нашем кэше C есть k страниц. Теперь мы хотим обработать последовательность из m страниц запросов, которые должны были быть помещены в кэш перед их обработкой. Конечно, если $m \leq k$, то мы просто помещаем все элементы в кэш, и это будет работать, но обычно мы имеем дело с $m>>k$.

\vspace{\baselineskip}

Мы говорим, что запрос является \textbf{кэш-попаданием}, когда страница уже находится в кэше, в противном случае, его называют \textbf{кэш-промахом}. В этом случае мы должны ввести запрашиваемую страницу в кэш и исключить другую, предполагая, что кэш заполнен. Результатом будет граф исключений, который \textbf{минимизирует количество исключений}.

\vspace{\baselineskip}

Существует множество стратегий для решения этой проблемы, давайте посмотрим на некоторые:

\begin{enumerate}
    \item \textbf{Первым вошел, первым вышел (FIFO)}: Самая первая страница исключается
    \item \textbf{Последним вошел, первым вышел (LIFO)}: Самая последняя страница исключается
    \item \textbf{Наименее раннее пользование (LRU)}: Исключение страницы, чей самый последний доступ был самым ранним
    \item \textbf{Наименее запрашиваемый (LFU)}: Исключение наименее запрашиваемой страницы (страницы, которая запрашивалась реже остальных)
    \item \textbf{Наибольшее расстояние вперед (LFD)}: Исключить страницу в кэше, которая не запрашивается до дальнейшего будущего
    \item \textbf{Очистка при заполнении (FWF)}: Очистка кэша происходит как только произошел кэш-промах
\end{enumerate}

Есть два способа решить эту проблему:

\begin{enumerate}
    \item \textbf{автономный}: последовательность запросов страниц известна заранее.
    \item \textbf{онлайн}: последовательность запросов страниц неизвестна заранее
\end{enumerate}

\textbf{Автономный подход}

\vspace{\baselineskip}

Для первого случая рассмотрим применение жадной стратегии. Этот третий пример \textbf{автономного кэширования} рассматривает все вышеприведенные стратегии и дает хорошую отправную точку для дальнейшего понимания.

\vspace{\baselineskip}

Пример программы был дополнен стратегией \textbf{FWF}:

\begin{tcolorbox}
\begin{minted}{C++}
class FWF : public Strategy {
public:
	FWF() : Strategy("FWF") //вызов конструктора класса Strategy
	{
	}
/*возвращение индекса кэша, с какого можно вставлять элементы*/
	int apply(int requestIndex) override 
	{
		for(int i=0; i<cacheSize; ++i)
		{
			if(cache[i] == request[requestIndex])
				return i;
  // после первой пустой страницы, все остальные тоже должны быть пусты
			else if(cache[i] == emptyPage)
				return i;
		}
		// нет пустых страниц
		return 0;
	}
	void update(int cachePos, int requestIndex, bool cacheMiss) override	{
		// нет пустых страниц -> пропустить -> очистить кэш
		if(cacheMiss && cachePos == 0)
		{
			for(int i = 1; i < cacheSize; ++i)
				cache[i] = emptyPage;
		}
	}
 }

\end{minted}
\end{tcolorbox}

Полный исходный код доступен \href{https://pastebin.com/AF7EC2xJ}{\underline{здесь}}. Если мы повторно используем пример из заголовка, то мы получим следующий вывод:

\begin{tcolorbox}
{\usefont{T2A}{cmtt}{m}{n}
Strategy: FWF

\vspace{\baselineskip}

Cache initial: (a,b,c)

\vspace{\baselineskip}
\begin{tabular}{*{5}{c}}
\textcolor{Blue}{Request} & cache \textcolor{Purple}{0} & cache \textcolor{Purple}{1} & cache \textcolor{Purple}{2} & cache miss \\
a & a & b & c & \ \\
a & a & b & c & \ \\
d & d & X & X & x \\
e & d & e & X & \ \\
b & d & e & b & \ \\
b & d & e & b & \ \\
a & a & X & X & x \\
c & a & c & X & \ \\
f & a & c & f & \ \\
d & d & X & X & x \\
e & d & e & X & \ \\
a & d & e & a & \ \\
f & f & X & X & x \\
b & f & b & X & \ \\
e & f & b & e & \ \\
c & c & X & X & x \\
\end{tabular}

\vspace{\baselineskip}

Total cache misses: \textcolor{Purple}{5}}
\end{tcolorbox}

Несмотря на то, что \textbf{LFD} является оптимальным, \textbf{FWF} имеет меньше кэш-промахов. Но главная цель заключалась в том, чтобы свести к минимуму количество исключений, а для \textbf{FWF} пять промахов означают 15 исключений, что делает его самым плохим выбором для данного примера.

\vspace{\baselineskip}

\textbf{Онлайн подход}

\vspace{\baselineskip}

Теперь мы хотим подойти к онлайн пейджирования. Но сначала нам нужно понять, как это сделать. Очевидно, что онлайн алгоритм не может быть лучше оптимального автономного алгоритма. Но насколько он хуже? Для ответа на этот вопрос нужны формальные определения:

\vspace{\baselineskip}

\textbf{Определение 1.1}: \textbf{Задача оптимизации} $\Pi$ состоит из набора \textbf{экземпляров} $\sum \Pi$. Для каждого экземпляра $\sigma \in \sum \Pi$ имеется набор $Z\sigma$ \textbf{решений} и \textbf{целевая функция} $f\sigma:Z\sigma\rightarrow\mathcal{R}\geq0$, которая присваивает каждому решению аппозитивное вещественное значение. Мы говорим, что $OPT(\sigma)$ - это значение оптимального решения, $A(\sigma)$ - это решение алгоритма $A$ для задачи $\Pi$ и $wA(\sigma)=f\sigma(A(\sigma))$ является его значением.

\vspace{\baselineskip}

\textbf{Определение 1.2}: Онлайн алгоритм $A$ для задачи минимизации $\Pi$ имеет \textbf{конкурентное соотношение} $r \geq 1$, если имеется постоянная $\tau\in\mathcal{R}$ такая, что

\vspace{\baselineskip}

\pline{0.4}{$wA(\sigma)=f\sigma(A(\sigma))\leq r\cdot OPT(\sigma)+\tau$}

\vspace{\baselineskip}

для всех экземпляров $\sigma\in\sum\Pi$. $A$ называется \textbf{r-конкурентным} онлайн алгоритмом. Если даже

\vspace{\baselineskip}

\pline{0.4}{$wA(\sigma)\leq r\cdot OPT(\sigma)$}

\vspace{\baselineskip}

для всех экземпляров $\sigma\in\sum\Pi$, тогда $A$ называется \textbf{строго r-конкурентным} онлайн алгоритмом.

\vspace{\baselineskip}

Таким образом, вопрос в том, насколько \textbf{конкурентным} является наш онлайн-алгоритм по сравнению с оптимальным автономным алгоритмом. В знаменитой \href{http://www.cs.technion.ac.il/~rani/book.html}{\underline{книге}} Аллана Бородина и Ран Эль-Янива использовался другой сценарий для описания ситуации с онлайн пейджированием:

\vspace{\baselineskip}

Есть \textbf{злостный враг}, который знает ваш алгоритм и оптимальный автономный алгоритм. На каждом шаге он пытается запросить страницу, которая является самой худшей для вас и одновременно лучшей для автономного алгоритма. \textbf{Конкурентный фактор} вашего алгоритма - это фактор того, насколько плохо ваш алгоритм справился с оптимальным автономным алгоритмом противника. Если вы хотите попробовать быть противником, вы можете попробовать \href{https://pastebin.com/u/kgoedde/1/Wak9refA}{\underline{Игру в противника}} (попробуйте побить стратегии подкачки страниц).

\vspace{\baselineskip}

\textbf{Алгоритмы маркировки}

\vspace{\baselineskip}

Вместо того, чтобы анализировать каждый алгоритм по отдельности, давайте посмотрим на специальное семейство онлайн алгоритмов для проблемы пейджирования, называемое \textbf{алгоритмами маркировки}.

\vspace{\baselineskip}

Пусть $\sigma = (\sigma 1, \dots, \sigma p)$ - экземпляр для нашей проблемы и k - размер нашего кэша, тогда $\sigma$ можно разделить на фазы:

\begin{itemize}
    \item Фаза 1 - это максимальная подпоследовательность $\sigma$ от начала до максимального k различных страниц
    \item Фаза $i \geq 2$ - это максимальная подпоследовательность $\sigma$ с конца фазы i-1 до максимального k различных запрошенных страниц.
\end{itemize}

Например, для k = 3:

\vspace{\baselineskip}

\pline{1}{\Large$\sigma = \Bigg(\overbrace{a,b,d,a}^{\text{фаза 1}},\overbrace{e,a,f,a,f}^{\text{фаза 2}},\overbrace{b,d,a}^{\text{фаза 3}},\overbrace{c,c,d}^{\text{фаза 4}}\Bigg)$}

\vspace{\baselineskip}

Алгоритм маркировки (прямо или косвенно) устанавливает, маркирована страница или нет. В начале каждой фазы все страницы не помечены. Если запрашивается страница во время фазы, она помечается. Алгоритмы маркировки никогда не исключают помеченную страницу из кэша. Это означает, что страницы, которые используются во время фазы, не будут исключены.

\vspace{\baselineskip}

\textbf{Теорема 1.3}: \textbf{LRU} и \textbf{FWF} являются алгоритмом маркировки.

\vspace{\baselineskip}

\textbf{Доказательство:} В начале каждой фазы (за исключением первой) \textbf{FWF} кэш-промах и очищает кэш. Это означает, что у нас есть k пустых страниц. В каждой фазе максимум k различных запрашиваемых страниц, так что теперь будет исключение во время фазы. Значит, \textbf{FWF} - это алгоритм маркировки. 

\vspace{\baselineskip}

Предположим, что \textbf{LRU} не является алгоритмом маркировки. Тогда есть экземпляр $\sigma$, где \textbf{LRU} помеченная страница x в фазе i исключена. Пусть $\sigma t$ сделает запрос в фазе i, где Х исключается. Так как x отмечен, то должен быть более ранний запрос $\sigma t^*$ для x в той же фазе, так что $t^* < t$. После $t^*$ x - самая новая страница в кэше, так что для того, чтобы исключить t, последовательность $\sigma t^* + 1, \dots, \sigma t$ должна запрашивать по крайней мере k из x разных страниц. Это означает, что фаза i запросила по крайней мере k+1 разных страниц, что противоречит определению фазы. Поэтому \textbf{LRU} должен быть алгоритмом маркировки.